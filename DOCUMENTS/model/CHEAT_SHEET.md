# OOD Detection Visual Guide & Cheat Sheet

## ğŸ¯ Decision Tree: When to Use What

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Product Recognition Request                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Has historical data?       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚          â”‚
            YES  â”‚          â”‚  NO
                 â†“          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Run LSTM     â”‚  â”‚ Use Default     â”‚
        â”‚ Model        â”‚  â”‚ Fallback (100)  â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                     â”‚
             â†“                     â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Confidence > 0.7 &                   â”‚
    â”‚ OOD Score < 0.6?                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                    â”‚
          YESâ”‚                    â”‚NO
             â†“                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ âœ… Use LSTM      â”‚  â”‚ âš ï¸  Use Fallback â”‚
    â”‚ Prediction       â”‚  â”‚ Strategy         â”‚
    â”‚ High Confidence  â”‚  â”‚ Medium Conf (0.5)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” OOD Detection Methods At a Glance

### 1. MC Dropout (Uncertainty Estimation)
```
Concept: Forward pass with dropout enabled multiple times
Formula: uncertainty = std(predictions across passes)

Code:
    predictions = []
    for _ in range(100):
        pred = model(X, training=True)  # dropout ON
        predictions.append(pred)
    std = np.std(predictions)
    
Result:
    std < 50   â†’ Confident (use LSTM)
    std > 200  â†’ Uncertain (use fallback)
    
Pros: âœ… Simple, built-in to existing model
Cons: âŒ Requires 50+ forward passes
```

### 2. Mahalanobis Distance
```
Concept: Distance from feature to training distribution
Formula: d = âˆš[(x-Î¼)áµ€ Î£â»Â¹(x-Î¼)]

Code:
    detector = OODDetector(method='mahalanobis')
    detector.fit(training_features)
    distance = detector.score(test_features)
    
Result:
    distance < 2   â†’ ID (known product)
    distance > 5   â†’ OOD (new product)
    
Pros: âœ… Single forward pass, good accuracy
Cons: âŒ Need to fit on training data
```

### 3. Energy-Based
```
Concept: Model as energy function
Formula: E(x) = -log(Î£ e^logit_i)

Code:
    energy = -log(exp(logits).sum())
    ood_score = sigmoid(energy)
    
Result:
    energy < -5   â†’ ID
    energy > 0    â†’ OOD
    
Pros: âœ… Fast, uses existing logits
Cons: âŒ Requires discrete outputs
```

### 4. Maximum Softmax Probability (MSP)
```
Concept: Max probability in softmax distribution
Formula: max(softmax(logits))

Code:
    probs = softmax(logits)
    max_prob = max(probs)
    ood_score = 1 - max_prob
    
Result:
    max_prob > 0.8 â†’ ID
    max_prob < 0.5 â†’ OOD
    
Pros: âœ… Simplest, no training needed
Cons: âŒ Low accuracy on hard cases
```

---

## ğŸ“Š Comparison Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method       â”‚ Setup    â”‚ Speed  â”‚ Accuracy â”‚ Recommended  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MC Dropout   â”‚ Easy (1) â”‚ Slow   â”‚ Very Goodâ”‚ âœ… START     â”‚
â”‚ Mahalanobis  â”‚ Medium   â”‚ Fast   â”‚ Excellentâ”‚ âœ… NEXT      â”‚
â”‚ Energy       â”‚ Easy     â”‚ Fast   â”‚ Good     â”‚ â­ Good      â”‚
â”‚ MSP          â”‚ Trivial  â”‚ Instantâ”‚ Fair     â”‚ â­ Baseline  â”‚
â”‚ Ensemble     â”‚ Hard     â”‚ Slow   â”‚ Best     â”‚ ğŸš€ Advanced â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Understanding Uncertainty

### Low Uncertainty (High Confidence)
```
Predictions: [500, 498, 502, 501, 499]
Mean: 500
Std:  1.4
Confidence: 0.99 âœ…

â†’ Use LSTM prediction
â†’ Very likely correct
```

### High Uncertainty (Low Confidence)
```
Predictions: [200, 800, 100, 900, 150]
Mean: 430
Std:  350
Confidence: 0.55 âš ï¸

â†’ Use fallback strategy
â†’ Model not sure
```

### Extreme Uncertainty (Very Low Confidence)
```
Predictions: [10, 2000, 5, 1800, 20]
Mean: 767
Std:  900
Confidence: 0.09 âŒ

â†’ Use default fallback
â†’ Model completely confused
```

---

## ğŸ”„ Complete Decision Logic

```python
def predict_smart(product_name, lstm_model, ood_detector):
    """Intelligent prediction with OOD handling"""
    
    # 1. Get LSTM prediction
    lstm_pred = lstm_model.predict(X)
    
    # 2. Get uncertainty estimate
    mean, std = estimate_uncertainty(lstm_model, X)
    confidence = 1.0 / (1.0 + std / mean)
    
    # 3. Get OOD detection score
    features = lstm_model.extract_features(X)
    ood_score = ood_detector.score(features)
    
    # 4. Make decision
    if ood_score > 0.6 or confidence < 0.5:
        # OOD or uncertain
        if has_historical_data(product_name):
            pred = fallback_exponential_smoothing(history)
            conf = 0.5
            method = "fallback"
        else:
            pred = default_quantity
            conf = 0.3
            method = "default"
    else:
        # ID and confident
        pred = lstm_pred
        conf = confidence
        method = "lstm"
    
    return {
        'prediction': pred,
        'confidence': conf,
        'ood_score': ood_score,
        'method': method
    }
```

---

## ğŸ¨ Visual: How OOD Detection Works

### Training Data Distribution
```
LSTM Hidden States (64-dimensional space projected to 2D)

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Training Data (ID)      â”‚
        â”‚    Coca Cola, Fanta, etc    â”‚
        â”‚                             â”‚
        â”‚        â—‹ â—‹  â—‹  â—‹            â”‚
        â”‚      â—‹   â—‹   â—‹   â—‹          â”‚
        â”‚    â—‹   â—   â—‹   â—   â—‹        â”‚  â† Cluster of known products
        â”‚      â—‹   â—‹   â—‹   â—‹          â”‚
        â”‚        â—‹  â—‹   â—‹             â”‚
        â”‚                             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Test Data: Known Product
```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Test Data (ID)          â”‚
        â”‚    New Fanta (similar)      â”‚
        â”‚                             â”‚
        â”‚        â—‹ â—‹  â—‹  â—‹            â”‚
        â”‚      â—‹   â—‹ â–² â—‹   â—‹          â”‚
        â”‚    â—‹   â—   â—‹   â—   â—‹        â”‚  â† Still in cluster
        â”‚      â—‹   â—‹   â—‹   â—‹          â”‚  âœ… OOD Score: 0.2
        â”‚        â—‹  â—‹   â—‹             â”‚
        â”‚                             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Test Data: Unknown Product
```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Test Data (OOD)         â”‚
        â”‚    Totally New Product      â”‚
        â”‚                             â”‚
        â”‚        â—‹ â—‹  â—‹  â—‹            â”‚
        â”‚      â—‹   â—‹   â—‹   â—‹   â–²      â”‚
        â”‚    â—‹   â—   â—‹   â—   â—‹        â”‚  â† Far from cluster
        â”‚      â—‹   â—‹   â—‹   â—‹          â”‚  âŒ OOD Score: 0.9
        â”‚        â—‹  â—‹   â—‹             â”‚
        â”‚                             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ Performance Benchmarks

### AUROC (Area Under ROC Curve) - Higher is Better
```
Perfect Detection:     AUROC = 1.00 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%
Excellent:            AUROC = 0.90 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  90%
Good:                 AUROC = 0.80 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  80%
Fair:                 AUROC = 0.70 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  70%
Random:               AUROC = 0.50 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  50%

Your Target:          AUROC = 0.85 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘  85%
```

### FPR@95%TPR - Lower is Better
```
Perfect:    FPR = 0.0%  No false OOD alarms
Excellent:  FPR < 10%   Mostly correct
Good:       FPR < 20%   Acceptable
Fair:       FPR < 30%   Needs improvement
Random:     FPR = 50%   Useless
```

---

## ğŸ› ï¸ Implementation Checklist

### Before You Start
- [ ] Read GET_STARTED.md (5 min)
- [ ] Review OOD_README.md (10 min)
- [ ] Look at INTEGRATION_GUIDE.md (5 min)

### Tier 1 Implementation (MC Dropout)
- [ ] Add `predict_with_uncertainty()` to lstm_model.py
- [ ] Import numpy in lstm_model.py
- [ ] Test with sample data
- [ ] Update forecast_service.py to use uncertainty
- [ ] Check that confidence scores make sense

### Tier 2 Implementation (Mahalanobis)
- [ ] Copy utils/ood_detection.py
- [ ] Add feature extraction to lstm_model.py
- [ ] Fit OOD detector during training
- [ ] Add `predict_with_ood_detection()` method
- [ ] Update forecast_service.py to check OOD score
- [ ] Test with known vs unknown products

### Validation
- [ ] Run test_ood_detection.py
- [ ] Check AUROC > 0.85
- [ ] Verify fallback strategies work
- [ ] Test integration with real data
- [ ] Monitor in production for drift

---

## ğŸ“ Key Equations

### Confidence from Uncertainty
```
confidence = 1 / (1 + std/mean)

Example:
  mean = 500, std = 50
  confidence = 1 / (1 + 50/500) = 1 / 1.1 = 0.91 âœ…

  mean = 500, std = 500
  confidence = 1 / (1 + 500/500) = 1 / 2 = 0.50 âš ï¸

  mean = 500, std = 2000
  confidence = 1 / (1 + 2000/500) = 1 / 5 = 0.20 âŒ
```

### Mahalanobis Distance
```
d = âˆš[(x - Î¼)áµ€ Î£â»Â¹(x - Î¼)]

Where:
  x = test sample feature vector
  Î¼ = mean of training features
  Î£â»Â¹ = inverse covariance matrix

Interpretation:
  d < 2   â†’ Standard deviation away (ID)
  d > 5   â†’ Far from distribution (OOD)
```

### AUROC Formula
```
AUROC = P(score_ood > score_id)

Means:
  - Probability that OOD sample scored higher than ID sample
  - Range: [0, 1]
  - 0.5 = random, 1.0 = perfect
```

---

## ğŸ’¾ Code Templates

### Template 1: Add MC Dropout
```python
# Add to models/lstm_model.py

def predict_with_uncertainty(self, X, num_samples=50):
    predictions = []
    for _ in range(num_samples):
        pred = self.model(X, training=True)
        predictions.append(pred.numpy())
    
    predictions = np.array(predictions).squeeze()
    mean = np.mean(predictions)
    std = np.std(predictions)
    confidence = 1.0 / (1.0 + std / (mean + 1e-6))
    
    return mean, std, confidence
```

### Template 2: Add OOD Detection
```python
# Add to models/lstm_model.py

def predict_with_ood_detection(self, X):
    from utils.ood_detection import OODDetector
    
    prediction = self.model.predict(X)[0, 0]
    
    if self.ood_detector is None:
        return prediction, 0.0, False
    
    features = self.feature_extractor.predict(X)
    ood_scores = self.ood_detector.score(features)
    is_ood = ood_scores[0] > self.ood_detector.threshold
    
    return prediction, ood_scores[0], is_ood
```

### Template 3: Integrate in Forecast Service
```python
# Update services/forecast_service.py

def predict_import_quantity_smart(product_name, product_info):
    lstm_model = get_lstm_model()
    
    # Prepare input
    X = prepare_features(product_name, product_info)
    X = np.array(X).reshape(1, -1)
    
    # Get predictions
    mean_pred, std_pred, conf = lstm_model.predict_with_uncertainty(X)
    pred_lstm, ood_score, is_ood = lstm_model.predict_with_ood_detection(X)
    
    # Decide
    if is_ood or conf < 0.5:
        from utils.ood_detection import FallbackPredictor
        pred = FallbackPredictor.exponential_smoothing(
            get_historical_imports(product_name)
        )
        method = "fallback"
        confidence = 0.5
    else:
        pred = mean_pred
        method = "lstm"
        confidence = conf
    
    return {
        'quantity': int(pred),
        'confidence': confidence,
        'ood_score': ood_score,
        'method': method
    }
```

---

## ğŸš¨ Troubleshooting

### Problem: All predictions have low confidence
```
Symptom: confidence always < 0.3
Cause: Too much dropout or high variance in LSTM
Solution: Reduce dropout from 0.3 to 0.2
```

### Problem: OOD detection doesn't discriminate
```
Symptom: OOD score = 0.5 for both ID and OOD
Cause: Features too similar or detector not fitted
Solution: Check training_stats is not None, use Mahalanobis
```

### Problem: Fallback predictions too high/low
```
Symptom: Fallback always predicts 1000
Cause: Historical data not loaded correctly
Solution: Print history before fallback call
```

### Problem: Predictions inconsistent
```
Symptom: Same product gets different predictions
Cause: Model not seeded or dropout too high
Solution: Set np.random.seed(42) before inference
```

---

## ğŸ“š File Structure Reference

```
YOUR_PROJECT/
â”‚
â”œâ”€ GET_STARTED.md              â† You are here
â”œâ”€ OOD_README.md               â† Overview & quick ref
â”œâ”€ OOD_STRATEGIES.md           â† All theory & methods
â”œâ”€ INTEGRATION_GUIDE.md        â† Implementation steps
â”‚
â”œâ”€ utils/
â”‚  â””â”€ ood_detection.py         â† Production code
â”‚
â”œâ”€ models/
â”‚  â””â”€ lstm_model.py            â† Modify this
â”‚
â”œâ”€ services/
â”‚  â””â”€ forecast_service.py      â† Modify this
â”‚
â””â”€ test_ood_detection.py       â† Run this to test
```

---

## âœ… Success Criteria

After implementation, you should see:

```
âœ… Confidence scores between 0 and 1
âœ… Known products: confidence > 0.8
âœ… Unknown products: confidence < 0.6
âœ… AUROC > 0.85 on test set
âœ… Fallback strategies return reasonable values
âœ… Integration tests pass
âœ… No errors in forecast service
âœ… API returns confidence field
âœ… Unknown products don't crash system
âœ… Predictions improve for new products
```

---

## ğŸ¯ Next Step

**ğŸ‘‰ Start with INTEGRATION_GUIDE.md - Quick Start (30 min)**

You're ready to build! ğŸš€
